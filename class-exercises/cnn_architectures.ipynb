{"cells":[{"cell_type":"markdown","metadata":{"id":"oaU9_sOGzRD1"},"source":["# CNN Architectures\n","In this notebook we will explore standard CNN architectures using PyTorch and torchvision.\n","\n","You can find more information on how to finetune pretrained models [here](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dvfTDUXuzRD9"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","np.random.seed(42)\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","from PIL import Image\n","# !pip install torchsummary\n","from torchsummary import summary"]},{"cell_type":"markdown","metadata":{"id":"BDBhCwhszREA"},"source":["### CIFAR10 Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"w5IQVCmOzREB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [16:20<00:00, 173962.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data\\cifar-10-python.tar.gz to data\n","Files already downloaded and verified\n","Training size: 50000 \n","Validation size: 10000\n"]}],"source":["# since we are going to use pretrained CNNs (on ImageNet), we need to normalize our data \n","# according to the mean and std with which these networks were trained, i.e. with the statistics of the ImageNet dataset\n","data_aug = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ])\n","\n","training_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=data_aug,\n",")\n","validation_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=data_aug,\n",")\n","print(f\"Training size: {len(training_data)} \\nValidation size: {len(validation_data)}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ByDvrH3z1uOT"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n"]}],"source":["# get cpu or gpu device for training\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","# now we need to define a Dataloader, which allows us to automatically batch our inputs, do sampling and multiprocess data loading\n","batch_size = 64\n","num_workers = 2 # how many processes are used to load the data\n","\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n","validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)"]},{"cell_type":"markdown","metadata":{"id":"g1kRGiw_zREE"},"source":["### Training loop"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"VxSdayviCWk5"},"outputs":[],"source":["def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n","    if is_train:\n","      assert optimizer is not None, \"When training, please provide an optimizer.\"\n","      \n","    num_batches = len(dataloader)\n","\n","    if is_train:\n","      model.train() # put model in train mode\n","    else:\n","      model.eval()\n","\n","    total_loss = 0.0\n","    preds = []\n","    labels = []\n","\n","    with torch.set_grad_enabled(is_train):\n","      for batch, (X, y) in enumerate(tqdm(dataloader)):\n","          X, y = X.to(device), y.to(device)\n","\n","          # Compute prediction error\n","          pred = model(X)\n","          loss = loss_fn(pred, y)\n","\n","          if is_train:\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","          # Save training metrics\n","          total_loss += loss.item() # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached\n","\n","          probs = F.softmax(pred, dim=1)\n","          final_pred = torch.argmax(probs, dim=1)\n","          preds.extend(final_pred.cpu().numpy())\n","          labels.extend(y.cpu().numpy())\n","\n","    return total_loss / num_batches, accuracy_score(labels, preds)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DmsUVGS6C0O1"},"outputs":[],"source":["def train(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer):\n","  train_history = {'loss': [], 'accuracy': []}\n","  val_history = {'loss': [], 'accuracy': []}\n","  best_val_loss = np.inf\n","  print(\"Start training...\")\n","  for t in range(num_epochs):\n","      print(f\"\\nEpoch {t+1}\")\n","      train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n","      print(f\"Train loss: {train_loss:.3f} \\t Train acc: {train_acc:.3f}\")\n","      val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False)\n","      print(f\"Val loss: {val_loss:.3f} \\t Val acc: {val_acc:.3f}\")\n","\n","      # save model when val loss improves\n","      if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n","        torch.save(save_dict, model_name + '_best_model.pth')\n","\n","      # save latest model\n","      save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n","      torch.save(save_dict, model_name + '_latest_model.pth')\n","\n","      # save training history for plotting purposes\n","      train_history[\"loss\"].append(train_loss)\n","      train_history[\"accuracy\"].append(train_acc)\n","\n","      val_history[\"loss\"].append(val_loss)\n","      val_history[\"accuracy\"].append(val_acc)\n","      \n","  print(\"Finished\")\n","  return train_history, val_history"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"xr48TEVlzREH"},"outputs":[],"source":["def plotTrainingHistory(train_history, val_history):\n","    plt.subplot(2, 1, 1)\n","    plt.title('Cross Entropy Loss')\n","    plt.plot(train_history['loss'], label='train')\n","    plt.plot(val_history['loss'], label='val')\n","    plt.legend(loc='best')\n","\n","    plt.subplot(2, 1, 2)\n","    plt.title('Classification Accuracy')\n","    plt.plot(train_history['accuracy'], label='train')\n","    plt.plot(val_history['accuracy'], label='val')\n","\n","    plt.tight_layout()\n","    plt.legend(loc='best')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Custom CNN"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ConvolutionalNeuralNetwork(\n","  (layers): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n","    (3): ReLU()\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Dropout(p=0.25, inplace=False)\n","    (6): Flatten(start_dim=1, end_dim=-1)\n","    (7): Linear(in_features=6272, out_features=64, bias=True)\n","    (8): ReLU()\n","    (9): Dropout(p=0.5, inplace=False)\n","    (10): Linear(in_features=64, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["from torch.nn import Conv2d, MaxPool2d, Linear, Dropout, Flatten\n","# define a custom CNN architecture\n","# copy the class used in the previous notebook (CNN Introduction)\n","# and adapt it to CIFAR-10 image shape - 32x32 with 3 colour channels\n","class ConvolutionalNeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNeuralNetwork, self).__init__()\n","        self.pool_size = 2\n","        self.nb_filters = 32\n","        self.kernel_size = 3\n","        self.dropout = 0.25\n","\n","        self.layers = nn.Sequential(\n","            Conv2d(3, self.nb_filters, self.kernel_size),\n","            nn.ReLU(),\n","            Conv2d(self.nb_filters, self.nb_filters, self.kernel_size),\n","            nn.ReLU(),\n","            MaxPool2d(self.pool_size),\n","            Dropout(self.dropout),\n","            Flatten(),\n","            Linear(6272, 64), \n","            nn.ReLU(),\n","            Dropout(self.dropout * 2),\n","            Linear(64, 10),\n","        )\n","\n","    def forward(self, x):\n","        logits = self.layers(x)\n","        return logits\n","    \n","\n","custom_cnn = ConvolutionalNeuralNetwork()\n","\n","custom_cnn.to(device)\n","print(custom_cnn)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training...\n","\n","Epoch 1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:22<00:00,  9.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 2.265 \t Train acc: 0.152\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:11<00:00, 13.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 2.200 \t Val acc: 0.240\n","\n","Epoch 2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:22<00:00,  9.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 2.170 \t Train acc: 0.215\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:12<00:00, 12.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 2.082 \t Val acc: 0.287\n","\n","Epoch 3\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:25<00:00,  9.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 2.089 \t Train acc: 0.248\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:12<00:00, 12.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.990 \t Val acc: 0.317\n","\n","Epoch 4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:23<00:00,  9.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 2.021 \t Train acc: 0.271\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:12<00:00, 12.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.921 \t Val acc: 0.342\n","\n","Epoch 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:27<00:00,  8.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 1.966 \t Train acc: 0.293\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:11<00:00, 13.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.862 \t Val acc: 0.358\n","\n","Epoch 6\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:23<00:00,  9.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 1.919 \t Train acc: 0.311\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:11<00:00, 13.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.809 \t Val acc: 0.380\n","\n","Epoch 7\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:17<00:00, 10.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 1.867 \t Train acc: 0.329\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:12<00:00, 12.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.755 \t Val acc: 0.397\n","\n","Epoch 8\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:36<00:00,  8.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 1.823 \t Train acc: 0.347\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:17<00:00,  8.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.709 \t Val acc: 0.407\n","\n","Epoch 9\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:40<00:00,  7.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 1.782 \t Train acc: 0.358\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:12<00:00, 12.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.672 \t Val acc: 0.419\n","\n","Epoch 10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 781/781 [01:25<00:00,  9.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 1.751 \t Train acc: 0.369\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:11<00:00, 13.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val loss: 1.635 \t Val acc: 0.428\n","Finished\n"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABI4ElEQVR4nO3deXyc1X3v8c9PuzTaR7tkLV7lDdtgjLFpQgNJbPashABN0gRub8kNJOlCc3MT2ua2dKNJbpJSQkibQCCEkAIJlEBim82AFwzed8mLJGuz9l363T/OI81Ilm3ZljQz0u/9es1rRvM8M3PmCZmvf+ec5zyiqhhjjDHhJirUDTDGGGNGYwFljDEmLFlAGWOMCUsWUMYYY8KSBZQxxpiwZAFljDEmLFlAGWOMCUsWUCbsicinRWSziLSJSLWIvCAiV4SwPRUi0um1Z/D2vTG+dr2IfGGi2zgWIvJZEXkt1O0w5nRiQt0AY85ERL4C3Av8CfAi0AOsAW4ETvlxFZEYVe2bhKZdr6ovj/ebTmL7jQl7VkGZsCUiacDfAHep6tOq2q6qvar6nKr+ubfPfSLylIg8KiItwGdFpEBEnhWRRhE5ICJ3BL3nCq8aaxGREyLygPd8gvceDSLSJCKbRCT3PNr8WRF5TUT+WUROishhEVnrbfu/wB8A3wuuukREReQuEdkP7Peeu8Nre6P3XQqCPkNF5EsickhE6kXkn0QkSkTivP0XB+2bIyIdIpJ9jt9jlXcMmr37VSO+4yERafW+363e87NFZIP3mnoR+fm5Hj9jhlFVu9ktLG+4SqkPiDnDPvcBvcBNuH9wJQKvAD8AEoClQB3wAW//jcDt3uNkYKX3+H8AzwFJQDRwCZB6ms+sAK4+zbbPeu25w3uf/wlUAeJtXw98YcRrFHgJyPTa/wGgHrgYiAf+H/DKiP3XefsXA/sG39P73v8QtO/dwHNnaOtrozyfCZwEbsf1stzi/e0HfEALMM/bNx9Y6D1+HPjf3v8OCcAVof5vyG6RfbMKyoQzP1CvZ+/y2qiq/6WqA0AWsBr4S1XtUtVtwMPAH3n79gKzRSRLVdtU9c2g5/3AbFXtV9Utqtpyhs/8L6/SGrzdEbStUlV/qKr9wH/ifsTPVo39vao2qmoncCvwiKpuVdVu4K+Ay0WkNGj/f/D2PwJ8GxcieJ93i4iI9/ftwE/P8tkjXQvsV9Wfqmqfqj4O7AGu97YPAItEJFFVq1V1p/d8L1ACFHjH3sa3zAWxgDLhrAHIEpGzjZUeDXpcADSqamvQc5VAoff488BcYI/XdXWd9/xPcWNcT4hIlYj8o4jEnuEzb1LV9KDbD4O21Qw+UNUO72HyOX6HyqD3aMMdi8LT7F/pvQZVfQvoAK4UkXJgNvDsWT57pGGfH/QZharaDtyMGxOsFpHfeJ8D8BeAAG+LyE4R+eNz/FxjhrGAMuFsI9CN6747k+Al+auATBFJCXquGDgOoKr7VfUWIAf4B+ApEfGpG9v6a1VdAKwCriNQdY2n010+YOR3KBn8Q0R8uOrueNA+M4IeF3uvGfSfwG246ukpVe06xzYO+/ygzxg8hi+q6gdxleEe4Ife8zWqeoeqFuC6TH8gIrPP8bONGWIBZcKWqjYD3wC+LyI3iUiSiMSKyFoR+cfTvOYo8Abw997Eh4twVdOjACJym4hke92BTd7LBkTkD0VksYhE48ZYenFdWePtBDDzLPs8DnxORJaKSDzwd8BbqloRtM+fi0iGiMzAjTMFT0h4FPgILqR+cpbPEu84Dd2A54G54qb3x4jIzcAC4NcikisiN3qh2Q204R0nEfmEiBR573sSF7oTcQzNNGEBZcKaqv4L8BXg67jJDkeBLwL/dYaX3QKU4iqBXwHf1MCU8DXAThFpA74DfMob98kDnsKF025gA2ceu3lOhp8H9asxfqXvAB/3Zvh9d7QdvLb+H+CXQDUwC/jUiN2eAbYA24DfAD8Kev1RYCsuIF49S3tWAZ0jbs24CvKruK7FvwCuU9V63G/GV3DHthF4P24iCMClwFvesX0WuFtVD53l8405rcGZRcaYCCEiCsxR1QNn2OcRoEpVvz55LTNmfNmJusZMMd5sv48Cy0LcFGMuiHXxGTOFiMjfAjuAf1LVw6FujzEXwrr4jDHGhCWroIwxxoSlkI1BZWVlaWlpaag+3hhjTJjYsmVLvaqesl5kyAKqtLSUzZs3h+rjjTHGhAkRGblyCWBdfMYYY8JURAdUbUsXNsnDGGOmpog9D6p/QLnmu6+RHB/NNYvzuWZxPgsLUgks4myMMSaSRXRAffVDc3l+ezX//sohfrD+ICX+JNYuyufaxfksKrSwMsaEv97eXo4dO0ZX17mu6Rt5EhISKCoqIjb2TBcKCAjZeVDLly/X8ZokcbK9h9/uquE322t440A9fQPKjMxErlnkKquLitIsrIwxYenw4cOkpKTg9/un9O+UqtLQ0EBraytlZWXDtonIFlVdPvI1EVtBBcvwxXHzpcXcfGkxTR09/HbXCZ7fXs2PXjvMv79yiML0RK5ZnMc1i/NZOiN9Sv9HYIyJLF1dXZSWlk753yURwe/3U1dXN+bXTImACpaeFMcnl8/gk8tn0NzRy2931fDCjhr+440KfvjqYQrTE1m7KI+1i/NZNiOdqKip/R+FMSb8TfVwGnSu33PKBVSwtKRYPrF8Bp9YPoPmzl5e3nWCF3ZU85ONlTz82mHy0xJYuyifaxbncXFxhoWVMcaEkbNOMxeRGSKyTkR2eZdxvnuUfW4VkfdEZLuIvCEiSyamuecvLTGWj11SxMOfuZTN/+dq/vXmJSwsSOPRtyr5+IMbWXX/77nv2Z28fbiRgQGbum6MmR6ampr4wQ9+cM6vu+aaa2hqahr/BgU56yQJEckH8lV1q3cZ7S3ATaq6K2ifVcBuVT0pImuB+1T1sjO973hOkrgQrV29/H5PLb95r5r1++ro6RsgJyWetYvcmNXy0kyirbIyxkyQ3bt3M3/+/JB9fkVFBddddx07duwY9nxfXx8xMePfyTba9z3vSRKqWo27qieq2ioiu4FCYFfQPm8EveRNoIjJUPEa5F0ECann/RYpCbHcuLSQG5cW0tbdx+/31PL8e9U8seko/7mxkuyUeNYsdGG1oszCyhgztdx7770cPHiQpUuXEhsbS0JCAhkZGezZs4d9+/Zx0003cfToUbq6urj77ru58847gcBydW1tbaxdu5YrrriCN954g8LCQp555hkSExMvuG3nNM3cuxDaK8AiVW05zT5/BpSr6hfO9F4XXEH1tMM/zwVVuOiTcOnnIW/x+b/fCO3dfazbW8vz26v5/Z5aunoHyEqO48ML87jWC6uY6IheiMMYEwaCK4q/fm4nu6pG/Wk9bwsKUvnm9QtPuz24glq/fj3XXnstO3bsGJoK3tjYSGZmJp2dnVx66aVs2LABv98/LKBmz57N5s2bWbp0KZ/85Ce54YYbuO222876fQdd8DRzEUkGfgncc4Zw+kPg88AVp9l+J3AnQHFx8Vg/enRxPvjMs7DpR/Du47DlxzBjpQuqBTdCTPwFvb0vPobrLirguosK6OjpY92eOp7fUc3TW4/z2FtH8Pvi+JAXVitnWlgZY6aGFStWDDtP6bvf/S6/+tWvADh69Cj79+/H7/cPe01ZWRlLly4F4JJLLqGiomJc2jKmgBKRWFw4PaaqT59mn4uAh4G1qtow2j6q+hDwELgK6rxaHKzwEnf70Ldg289g84/g6Tvgv/8KLr4dLvkcZJRc8MckxcVw7UX5XHtRPp09/azfW8tvtlfzzLbjPP72ETKSYvmw1w14+Sw/sRZWxpjzcKZKZ7L4fL6hx+vXr+fll19m48aNJCUlceWVV4664kV8fKAgiI6OprOzc1zactaAEjdx/Ue4SRAPnGafYuBp4HZV3TcuLTsXSZmw6ouw8k/h8HpXVb3+HXjt2zDnQ3DpF2D2VRAVfcEflRgXzdrF+axdnE9Xbz/r99bx/PZqnnu3iic2HSU9KZYPL8hj7eI8Vs3KIi7GwsoYE75SUlJobW0ddVtzczMZGRkkJSWxZ88e3nzzzUlt21gqqNXA7cB2EdnmPfc1oBhAVR8EvgH4gR94J2L1jdafOOGiomDWB9yt+Rhs+Q/Y8p/ws09Aegks/xwsux18WePycQmx0axZlMeaRXl09fbzyj4XVr/ZXs3PNx8lOT6G1bP9XDkvh/fPzaYg/cIHDY0xZjz5/X5Wr17NokWLSExMJDc3d2jbmjVrePDBB5k/fz7z5s1j5cqVk9q2KbEW3xn19cCeX7uqqvI1iI6DhR+B5Z+HGStgAs7g7urt57X99fxuTy0b9tZS1exK4rm5yUNhtbw0g/iYC6/ojDGRLdTTzCfbuUySmPoBFax2N2x+BLY9Dj2tkLvYTapY/AmIT56Qj1RVDtS2sX5vHev31fL24UZ6+5WkuGhWzcriynnZXDkvm6KMpAn5fGNMeLOAsoAarrsNtv8CNj0MJ3ZAfCosucWFVfa8Cf3o9u4+Nh5sYP2+WtbvrePYSTeYOCvbx5XzcrhyXjYryjKtujJmmrCAsoAanSocfdsF1a7/gv4eKP0DF1Tl10H02K5Zcv4frxysa2fDvjrW763lrcON9PQNkBgbzeWz/K66mptDsd+qK2OmKguoKX65jfMmAsWXuduav4d3fuq6AH/xWUjOhYs/A5d8FtIKJ+jjhdk5yczOSebzV5TR0dPHm4ca2LC3jvX76vj9nlpgJzOzfLxvrusKXDnTT0KsVVfGmKlveldQoxnohwMvu6pq/0sgUTBvrauqyq50MwUnyeH6dtbvrWXDvjo2Hmygu2+A+JgoLp/l5/1zs7lyXg5lWb6zv5ExJmxZBWVdfOfnZAVs/rGrrDoaIHOWC6qln4bEjEltSldvv6uu9tWxYW8dh+rbASjxJ3GlF1YrZ/pJjLPqyphIYgFlAXVh+rph1zOuqjr6FsQkwKKPu7AqvDgkTTrS0MH6fbVs2FvHGwcb6OztJy4misvKMoemss/K9k2bC6EZE6kiMaCSk5Npa2s7r9daQE2kmu3unKr3noTedihY5laqWPhRiAvNZIau3n42VTSyfm8dG/bVcaDW/YdTlJE4NNHi8ll+fPHTe8jRmHBkAWUBNf66muHdn7uqqn4vJKTDsttg+R+Df1ZIm3a0scObGVjHGwfr6ejpJy46ikvLMrhyrpvKPjsn2aorY8JAOATUvffey4wZM7jrrrsAuO+++4iJiWHdunWcPHmS3t5evvWtb3HjjTcCFlCRQxUqX3dBtfs5GOiDmX/ouv/mroXo0FYtPX0DbK5oZL03lX3fCfcfVV5qApeWZXJpaQaXlmYyLzfFLnlvTAgM+8F+4V7XSzOe8hbD2vvPuMs777zDPffcw4YNGwBYsGABL774ImlpaaSmplJfX8/KlSvZv38/IjJpAWV9PhdKBEqvcLfWGtj6U3fpj5/fBkl+KL8W5t8IZe+DmLhJb15cTBSrZmexanYWX7tmPlVNnWzY58atNh1u5Ll3qwBITYhheWkml5ZmsqIsg8WF6bbQrTHTxLJly6itraWqqoq6ujoyMjLIy8vjy1/+Mq+88gpRUVEcP36cEydOkJeXN2ntsgpqIvT3wf4XYccvYd+L0NMG8Wkwbw3Mvx5mXRWy8apgqsqxk528fbiRTRXudrDOzQ6Mj4li6Yx0VpS50Lq4JINkG8MyZtyFQxcfwDe+8Q2ysrKoqakhLy+P1NRUXnjhBR599FFiY2MpLS1l/fr1lJaWWgUV0aJjXOVUfi30dsGh9bD7WdjzG3jv5xCbBHM+CPNvcJcDuYBL1l8IEWFGZhIzMpP42CVFANS3dbO5opG3D59kU0Uj3193gAGF6ChhQX7qUIW1vDSTrOQLuyikMSZ83Hzzzdxxxx3U19ezYcMGnnzySXJycoiNjWXdunVUVlZOepssoCZabIKrnOatgf5eqHjNhdXuX7up69Fx7vIg86+Hede4a1uFUFZyPGsW5bNmUT4Abd19bK086UKropHH3qrkkdcPAzAz28eKoW7BTIoyEm3ihTERauHChbS2tlJYWEh+fj633nor119/PYsXL2b58uWUl5dPepusiy9UBvrdOoC7n3WTK5qPgkRD2R+4yqr8OkjJPfv7TLKevgG2H292XYJe12BLVx8QmHixotRVWDbxwpizC5cuvslis/gijSpUvePCatez0HgQEChe6Sqr+ddDenGoWzmqgQFlX20rmw438nbFSTYdbqSmxV3/yiZeGHN2FlAWUJFD1V23arCyOrHDPV+wzAurGyFrdmjbeAYjJ168XdHIIZt4YcxpWUBZQEWuhoOBsDq+xT2Xs8ALqxsgd+GEXBV4PI2ceLGzqtkmXhjj2b17N+Xl5dNi/FZV2bNnjwXUlNR8zAXV7ueg8g1AIXNmoLIqvDjswwoCEy82VTTy9uFGth1tortvAHATLy4pzmBRYRoLC1KZn59qSzSZKe3w4cOkpKTg9/undEipKg0NDbS2tlJWVjZs23kHlIjMAH4C5AIKPKSq3xmxjwDfAa4BOoDPqurWM72vBdQFaqt109Z3PwuHX3ErWKQWBcasildCVGSsbN7d18+O481DFdY7R05ysqMXcHlb5vexoCCVhQUutBYWpOK3SstMEb29vRw7doyurq5QN2XCJSQkUFRURGzs8IvBXkhA5QP5qrpVRFKALcBNqroraJ9rgP+FC6jLgO+o6mVnel8LqHHU0ehOCN79LBz4HfR3gy/bW8XiBreKxQRfHXg8qSo1LV3sPN7CzqoWdlY1s7OqheNNnUP75KUmDIXVAi+4bJq7MZFp3Lr4ROQZ4Huq+lLQc/8OrFfVx72/9wJXqmr16d7HAmqCdLe6Cy3ufhb2/datuJ6Q5s6xmn+DO+cqNiHUrTwvTR097KoaHloH69oY8P4TTkuMZUG+C62Fha7impnlIybaZg4aE87GJaBEpBR4BVikqi1Bz/8auF9VX/P+/h3wl6q6ecTr7wTuBCguLr4kFGcmTyu9nXBwnQurvc+7Fdjjkt0qFnPXQulqSCsKdSsvSGdPP3tqBkOrhV1VzeypaR0a04qPiaJ8MLS8bsLyvBQSYiOj+9OY6eCCA0pEkoENwP9V1adHbBtTQAWzCmqS9fVAxauBJZfa69zz6cVQshpKVkHxKnepkAjvJuvrH+BgXftQlTV43+qdUBwdJczK9g2NaS0oSGVhfhppSZHTDWrMVHJBASUiscCvgRdV9YFRtlsXXyQZ6HfnV1W+Ebh11LttvhwXVoOhlbMAoiK/i2zw/KxAaLngOtHSPbRPUUbiUJU1eJ+bGm/jWsZMsAuZJCHAfwKNqnrPafa5FvgigUkS31XVFWd6XwuoMKIK9fvdda0GA6vlmNuWkAbFlwdCK39JRE24OJv6tu5hVdauqhYO17cPbff74obNIFxQkEpJZpKNaxkzji4koK4AXgW2AwPe018DigFU9UEvxL4HrMFNM//cmbr3wAIq7DUd8cLKC62GA+752CQoujRQYRUth9jE0LZ1nLV197G7uoWdxwPV1v7aVnr73f9X4qKjmJntY3ZOMnNyUtx9bjKlfp8t5WTMebATdc2FaT0BRzYGKqwTOwCFqFgovARKLnehNWOFq7qmmJ6+AfadaGV3dQsH6to4cKKN/bVtHD3ZweD/haKjhFJ/EnNyUpiTmzwUYDOzfTYpw5gzsIAy46uzCY6+Faiwqt5xJwtLlLvE9NDEi8vBlxXq1k6Yzp5+DtW3caC2jf0n2thf28r+2jYqGzro9+a/RwnMyExiTk4ys3NSmONVXLOyk22VDGOwgDITracdjm0KVFjHNkGfd2Z81rygiReXR/zU9rHo7uunor6D/bWtLrxqXdV1qL5tqKsQoDA9kTm5yV54uQCbnZNMWuLUGecz5mwsoMzk6uuGqm2BCuvoW9DtnToXPLW9ZLVbT3CazJTr6x+gsrGD/SfaOOBVWwe82+C5WwC5qfHDxrcGH2f64kLYemMmhgWUCa1hU9u90OpocNuSc72ZglNravu56B9Qjp/sHOoiDA6wjp7+of38vrhhoTUnJ5nZuclkJ9t0eBO5LKBMeDllavvr0HLcbYtPg4Il7hpYBRe7+/TiaVNlBVNVqpu7vNAKdBfuP9E6dCVjcMs8zclJZl5eCvPzU5mfn8K8vFS73paJCBZQJrypuqntRza67sDjW+HEThhwq5qT5A+EVaF3n5IX2jaHkKpS19odCK66NvbVtLG7JrBiBkBxZhLlQaFVnpdKcWYSUVHTL+xN+LKAMpGnr9t1C1a9A8ffcfd1u0G9sZqUAi+wlgWqraTM0LY5xFSVquYu9lS3sLu6hd01reypdicfDy6qmxQXzbw8F1YL8lMoz09lXl4KqQk2McOEhgWUmRp62qFmu6uwqt6Bqq2Bk4gB0ksCFVbBxW7li4TU0LU3THT29LO/1p3Htbu6lT017r65s3don6KMxGGhVZ6XQonfR7RVW2aCWUCZqaur2c0YHAysqndcdyEAAllzho9n5S2GuKRQtjgsDF53KxBaLsAOBV3CJDE2mrl5Kcz3ugnL81x42TR4M54soMz00l7vhZYXWMe3QluN2ybRkDPfCy1vTCtnIcTYFG6Art5+DtS2sau6hT3VXtVV00JTR6DaKkxPHBrbKvfGtsqyrNoy58cCypiW6uGBVbUVOk+6bdFxkLto+CSM7HKIsiWKwFVbta3dQ6HlughbOFjXPrRiRnxMlDe2NVhtuYkZ6UkW/ObMLKCMGUkVmiqDxrPecVVXT6vbHpvkxrCCuwczZ067c7TOpLuvn/0n2tjjTcbY7Y1tNbb3DO2Tn5ZAeZ470XhWdjKzvHs76dgMsoAyZiwGBtyki+DxrOr3oK/TbY9PdZVW3iI3lpW3GLLnQ2xCaNsdRlSVurZuN67lzSbcU9PKofp2eoJWy0hPinWBle3z7pOZme2j2C5nMu1YQBlzvvr7oG5PoMo6sQNqdkCvd90oiYasuYHQyl0EeRdBcnZo2x1m+geUqqZODtS1cbC2jYN17Ryqc/f1bYELR8ZGCyV+H7Oyfcz0gmvwsU3OmJosoIwZTwMDcPKwm/J+Yoe7r9kRuNAjuCWchgJrsQst/ywb1xpFc0cvB+vbOFTXzkEvwA7Vt1NR307fQOA3KjslnplZvqFuwsHqqzA90U4+jmAWUMZMho7G4YFVs91VX4MrYsQkQu6CoNBaDLkLIT4ltO0OU739Axxt7OCgF1yDFdeB2rZh53DFx0RRNkpwlWX57JImEcACyphQ6euB+r2BwDqx3d0PziAEyCgLBNZg1ZVWNC3XHxwLVaWxvYdD9e1ed2Ggy/BIYwdBRRcFaQleV2FwgCWTm2oL7IYLCyhjwomqWxx3ZGg1Hgrsk5B+amhll9v5WmfR3ddPZUPHUDdhcIC1dQfWKfTFRQ8FV2mWj7IsH6V+99jGuibXeQeUiDwCXAfUquqiUbanAY8CxUAM8M+q+uOzNcgCyphRdLfCiV2BwKrZ4RbNHZxFGBUL2fNGjG0tnvZrEI7F4AK7B+qGT9A4WNtGVXMnwT+Fmb44Sv1JQ4FVmuWjzO+jNCuJFFuzcNxdSEC9D2gDfnKagPoakKaqfyki2cBeIE9Ve0buG8wCypgxGuh3lVXNe0EV1w5orQ7sk1roAiv/IjcZI28xZJRaF+EYdfX2c7Sxg8P17VQ0tHO4voMK73F1c9ewfbOS4yj1+yjx+yjLSnIB5gWZXd7k/JwuoM56NFX1FREpPdMuQIq4ztxkoBHoO8P+xphzERXt1hPMmgOLPhZ4vr3eq7K8wKp+Dw68DOpd4DA+zQXVYGjlX+Smw0dbBTBSQmw0c3JTmJN76mSVzp5+jgSFV0V9O4fr23ntQB2/3No9bN/slPhhlVeg2zCJpDgLr3M1pjEoL6B+fZoKKgV4FigHUoCbVfU3p3mfO4E7AYqLiy+prKw8/5YbY07V2wm1u1xY1bwX6CYc7CKMjnfrEA6F1hI3izDOF9p2R6iOnj4qG1y1ddgLr4r6Dg43tFPXOjy8clPjXdU1FF6u+irJ9JEYN71PPbigSRJnCaiPA6uBrwCzgJeAJaracqb3tC4+YybJQL9bHaP6Pah5NxBeQ7MIBfyzh1daeUvA5w9psyNdW3cflQ0usFy3YftQt2F92/ARkLzUBEqzkoZN1CjLcqtqJMRO/fA67y6+MfgccL+6pDsgIodx1dTb4/DexpgLFRXtJlZkz4OLPuGeG5xFOBhW1e/B0bdhxy8Dr0spGBFaF0F6sY1rjVFyfAwLC9JYWJB2yrbWrl4qGzqGQmuw+npx54lh6xiKQFZyPPlpCeSmJgy7z0tNIC/N3aZq9+F4fKsjwFXAqyKSC8wDDp35JcaYkBJx51mlFUH5NYHnOxq9bsH3AuG1/7eBqxgnpHmTMIJCK2suRE/NH8iJkpIQy6LCNBYVnhpezZ29VA5VXB1UNXVS09LFkYYO3jrUQEvXqUP8qQkx5KclkpuWQH5qgrsPDrHUBNKTYiPuvK+xzOJ7HLgSyAJOAN8EYgFU9UERKQD+A8gHBFdNPXq2D7YuPmMiRG+nm/oe3D14Yif0ebPbYhIgZ0HQhIzBcS27KORE6Ojpo6a5i5qWruH3QY/r2roZ+dMeHxM1FFaDlVfesKoskeyU+JBc08tO1DXGjJ/+PjeuVfMeVL8bqLi6mtx2iQL/nOGzCHPmu/UJI+xf8ZGot3+AutZuqpu7ONHSNey+ptlVZCeau+npHxj2uiiBnJSEoUosOMjy0gJhNt7jYhZQxpiJpQrNR10XYfDYVvACuvFpbrp89jxv6rw3NpZeYt2Ek2xwuajRQmzovrmL1u5TuxQzkmLJTU3g8ll+vnn9wgtuy0ROkjDGGFcZpRe7W/m1gefbG9zKGHX73JqE9fvgwO9g22OBfaLjIHMWZM91Y1pZ89xj/xzrKpwgIoI/OR5/cvyoY2GD2rr7RnQhdg51JUZPcDVsFZQxJjQ6m1w3Yd1eL7j2u8cnDwcmZQCkFXvBNS+o+ppn0+CnEKugjDHhJTEdipa7W7C+bre0U51XbQ3eV7weOOEYIDHTCyuv6hp8nDYDouyKvFOBBZQxJrzEeKtd5Mwf/vzAgBvjqt/vKq46r+ra82voaAh6fSJkzfYqrrmB6ss/y723iRgWUMaYyBAVBRkl7jbn6uHb2htclVW/1xvr2gfH3oYdTwX2kSi3gO7IrsKsOa6aM2HHAsoYE/l8fvBdDiWXD3++p8ONcw11FXpV18HfQX/QckOJmS740kvcJI+MEkgvdfdpMyA2YVK/jnEsoIwxU1dckjsPK/+i4c/390FTZSC4Tla4v2u2w97nh4cXQEr+iPAqCdynFtoU+QliR9UYM/1Ex7gxKf8smLd2+LaBAWirgZOVLrSC74+86boNg2cZSjSkFQaFVunwEPPl2KSN82QBZYwxwaKiILXA3UZ2GQL090LzsVPDq+kI7H8J2k4M3z8mwXUTjqy8Bu8TM2x1jdOwgDLGmHMRHQuZZe42mt5OF1ZD4VXh/m6qhGObA8tBDYpLOTW0grsS45Mn+huFLQsoY4wZT7GJgcubjKar+dTKq6nSnft1aB30dgzfPynLm71Y6oVYaeDv1KIpPf41db+ZMcaEo4S00SdugFvPsL3eBdZg9TUYZse3wK5nYCBobTyJdpdMCQ6tjFJvHKwUkjIjuvvQAsoYY8KFCCRnu9vIFTbAzT5sOT48vE5WuNveF6C9bvj+cckjKq/SoGqs2FV7YcwCyhhjIkV0TOBk5bL3nbq9u80b/6oYHmKn6z5MzhtReQV1IaYUhHz2oQWUMcZMFfHJkLvA3UZSdRVWcOXV5D2u3AjbfzF8+nx0nLc6/Yhxr8G/J2H1DQsoY4yZDkQgOcfdZqw4dXtfj1vrcGT3YVMlVG2FzpPD909Ih7kfho8+NGFNtoAyxhgDMXGBk5dHM2z2YYW7pRZMbJMm9N2NMcZMDWeafThBbP0NY4wxYckCyhhjTFgK2SXfRaQOqByHt8oC6sfhfaYbO27nx47b+bNjd36mw3ErUdXskU+GLKDGi4hsHu1a9ubM7LidHztu58+O3fmZzsfNuviMMcaEJQsoY4wxYWkqBNTEnSU2tdlxOz923M6fHbvzM22PW8SPQRljjJmapkIFZYwxZgqygDLGGBOWIjagRGSNiOwVkQMicm+o2xMpRGSGiKwTkV0islNE7g51myKJiESLyDsi8utQtyVSiEi6iDwlIntEZLeIXB7qNkUCEfmy9//RHSLyuIgkhLpNky0iA0pEooHvA2uBBcAtIjLK+vJmFH3AV1V1AbASuMuO3Tm5G9gd6kZEmO8A/62q5cAS7PidlYgUAl8ClqvqIiAa+FRoWzX5IjKggBXAAVU9pKo9wBPAjSFuU0RQ1WpV3eo9bsX9WBSGtlWRQUSKgGuBh0PdlkghImnA+4AfAahqj6o2hbRRkSMGSBSRGCAJqApxeyZdpAZUIXA06O9j2I/sORORUmAZ8FaImxIpvg38BTBwlv1MQBlQB/zY6xp9WER8oW5UuFPV48A/A0eAaqBZVX8b2lZNvkgNKHOBRCQZ+CVwj6q2hLo94U5ErgNqVXVLqNsSYWKAi4F/U9VlQDtgY8ZnISIZuF6hMqAA8InIbaFt1eSL1IA6DswI+rvIe86MgYjE4sLpMVV9OtTtiRCrgRtEpALXpfwBEXk0tE2KCMeAY6o6WKU/hQssc2ZXA4dVtU5Ve4GngVUhbtOki9SA2gTMEZEyEYnDDR4+G+I2RQQREdx4wG5VfSDU7YkUqvpXqlqkqqW4/95+r6rT7l+050pVa4CjIjLPe+oqYFcImxQpjgArRSTJ+//sVUzDySUReUVdVe0TkS8CL+JmtzyiqjtD3KxIsRq4HdguItu8576mqs+HrklmivtfwGPePyYPAZ8LcXvCnqq+JSJPAVtxM2/fYRoueWRLHRljjAlLkdrFZ4wxZoqzgDLGGBOWLKCMMcaEJQsoY4wxYckCyhhjTFiygDLGGBOWLKCMMcaEJQsoY4wxYckCyhhjTFiygDLGGBOWLKCMMcaEJQsoY4wxYckCyoQVEblvIq+zJCI7ReRK77GIyI9F5KSIvC0ifyAieyfgM4tFpE1Eosf7vY2ZyiygzKQTkU+LyGbvR7taRF4QkSsm47NVdaGqrvf+vAL4IFCkqitU9VVVnXf6V4+NiFSIyNVBn3lEVZNVtf9C3/s0nycickhE7DpLZkqxgDKTSkS+Anwb+DsgFygGfoC7vPVkKwEqVLU9BJ89nt4H5AAzReTSyfxgEYnIa8qZyGABZSaNiKQBfwPcpapPq2q7qvaq6nOq+uenec0vRKRGRJpF5BURWRi07RoR2SUirSJyXET+zHs+S0R+LSJNItIoIq+KSJS3rUJErhaRzwMPA5d7ldxfi8iVInIs6P1niMjTIlInIg0i8j3v+Vki8nvvuXoReUxE0r1tP8WF7nPe+/6FiJSKiA7+mItIgYg867XtgIjcEfSZ94nIkyLyE+977RSR5Wc5tJ8BngGe9x4HH7+FIvKS91knRORr3vPRIvI1ETnofc4W7/sOa6u373oR+YL3+LMi8rqI/KuINAD3nel4nO44ikic16bFQfvliEiHiGSf5fuaacICykymy4EE4Ffn8JoXgDm4CmEr8FjQth8B/0NVU4BFwO+9578KHAOycVXa14BhV+ZU1R8BfwJs9Lrfvhm83Rsv+jVQCZQChcATg5uBvwcKgPnADOA+731vx12u+3rvff9xlO/0hNe+AuDjwN+JyAeCtt/g7ZMOPAt873QHR0SSvPd4zLt9StyVaxGRFOBl4L+9z5oN/M576VeAW4BrgFTgj4GO033OCJfhroybC/xfznA8TnccVbXH+463Bb3vLcDvVLVujO0wU5wFlJlMfqBeVfvG+gJVfURVW1W1G/ejt8SrxAB6gQUikqqqJ1V1a9Dz+UCJV6G9qud+6egVuB/cP/cqvS5Vfc1r0wFVfUlVu70f0weA94/lTUVkBrAa+EvvPbfhKrk/CtrtNVV93huz+imw5Axv+VGgG/gt8BsgFrjW23YdUKOq/+J9VquqvuVt+wLwdVXdq867qtowlu8AVKnq/1PVPlXtPMvxOO1xBP4TuEVExPv7du/7GgNYQJnJ1QBkjXXcwuuGut/rhmoBKrxNWd79x3AVQKWIbBCRy73n/wk4APzWmzxw73m0dQZQOVqYikiuiDzhdSu2AI8GtelsCoBGVW0Neq4SV1kMqgl63AEknOGYfQZ40guLLuCXBLr5ZgAHT/O6M207m6PBf5zleJz2OHph2QFcKSLluArv2fNsk5mCLKDMZNqI+9f+TWPc/9O4yRNXA2m4LiJwXUqo6iZVvRHX/fdfwJPe862q+lVVnYnrLvuKiFx1jm09ChSfJhj+DtdluFhVU3HdVBK0/UzVWhWQ6XW/DSoGjp9j+xCRIuADwG3eOF0NrrvvGhHJ8r7DzNO8/Cgwa5TnByeMJAU9lzdin5Hf70zH40zHEVwVdRuuenrKC1ljAAsoM4lUtRn4BvB9EblJRJJEJFZE1orIaGM1KbhAa8D9YP7d4AZvkP1WEUlT1V6gBRjwtl0nIrO9rqNmoH9w2zl4G6gG7hcRn4gkiMjqoHa1Ac0iUgiMnOBxgtMEg6oeBd4A/t57z4uAz+OqjnN1O7APmAcs9W5zceNbt+DGfvJF5B4RiReRFBG5zHvtw8DfisgccS4SEb/XRXccF3rRIvLHjB5kwc50PM50HPG+90dwIfWT8zgGZgqzgDKTSlX/BTdA/3WgDvcv7C/iKqCRfoLr/joO7ALeHLH9dqDC61b6E+BW7/k5uMkBbbiq7Qequu4c29kPXI/rdjqC+9G/2dv818DFuPD7DfD0iJf/PfB1cbMI/2yUt78FVw1W4SaMfFNVXz6X9nk+g/tuNcE34EHgM1434ge971ED7Af+0HvtA7iK87e4cP8RkOhtuwMXMg3AQlygnslpj8dZjuNgYG/FVWCvnvshMFOZnPvYsTHGjB8ReQQ38eLroW6LCS92kp0xJmREpBQ3E3FZiJtiwpB18RljQkJE/hbYAfyTqh4OdXtM+LEuPmOMMWHJKihjjDFhKWRjUFlZWVpaWhqqjzfGGBMmtmzZUq+qp6zBGLKAKi0tZfPmzaH6eGOMMWFCRCpHe966+IwxxoQlm2ZujDHm9FShuwU6T556S/LDwo9M2EdbQBljzHTQ3wddzaMHzeCtq2mU55vgdBeDLlk9fQKqt7eXY8eO0dU19deLTEhIoKioiNjY2FA3xRgTSfq6XWicKWhGC5nu5jO/b0IaJGYEbunFgccJ6cO3Dd3SJ/SrhlVAHTt2jJSUFEpLSwlcImbqUVUaGho4duwYZWVloW6OMSaU+nuhvQ7aTkBbrXc/+LgWOhqGB1Jv++nfS6KGB0hyDmTPO024BN0S0iAqetK+8liFVUB1dXVN+XACEBH8fj91dXbhUGOmpIEB6GwMCpu6EcETdN/ZOPp7JKSBLwd82a6ayV/iKpbTBk06xKVA1NSZ+xZWAQVM+XAaNF2+pzFThip0t45S5ZyA9toRwVM7+rhNTCKk5Lrg8c+CklWQnOsqneTcwGNfNsQmTP53DDNhF1DGGDOpejtdoLSfocoZrIL6Ok99fVSMC5zBkMlbPDxsgh/HJYP943TMxnrp7TXAd4Bo4GFVvf80+30MeAq4VFUj7izcpqYmfvazn/Gnf/qn5/S6a665hp/97Gekp6dPTMOMMWPX3+fGbdrr3C34cXsdtNcH3ddDT+vo75PkDwRL8eWuqhkteBIzplS3Wjg5a0CJSDTwfdyFz44Bm0TkWVXdNWK/FOBu4K2JaOhkaGpq4gc/+MEpAdXX10dMzOkP1fPPPz/RTTNm+lJ1kwPa66GjfpSgqYP2oBA63ZiORIMvywVNkh8KL3GPff5Tg8eXDdE2wzbUxlJBrQAOqOohABF5ArgRd4XTYH8L/AOnXv46Ytx7770cPHiQpUuXEhsbS0JCAhkZGezZs4d9+/Zx0003cfToUbq6urj77ru58847gcCyTW1tbaxdu5YrrriCN954g8LCQp555hkSExPP8snGTCOq0NM+tuqmvc6F0kDf6O+VmOEFTpabrVZ6hRc6WSPus91Uaat0IspYAqoQd1nuQceAy4J3EJGLgRmq+hsROW1AicidwJ0AxcXFZ/zQv35uJ7uqWsbQvLFbUJDKN69feNrt999/Pzt27GDbtm2sX7+ea6+9lh07dgxNBX/kkUfIzMyks7OTSy+9lI997GP4/f5h77F//34ef/xxfvjDH/LJT36SX/7yl9x2223j+j2MCWs97XCyEk5WBG5Nld44jxc6o43lgBujGQyVtCIoWBoImKHACaqCrMqZ0i54koSIRAEPAJ89276q+hDwEMDy5cvD/kJUK1asGHae0ne/+11+9atfAXD06FH2799/SkCVlZWxdOlSAC655BIqKiomq7nGTI6BfmipcqETHEKDt/YRp0/EpUBGies6y5o7vKoZ7GIbrILikib965jwNZaAOg7MCPq7yHtuUAqwCFjvTZ3OA54VkRsuZKLEmSqdyeLz+YYer1+/npdffpmNGzeSlJTElVdeOeqKF/Hx8UOPo6Oj6ew8zb8UjQlnXc2nVkFD1dARGOgN7CtRrtrJKIV5a9390K3MdcPZzDVzHsYSUJuAOSJShgumTwGfHtyoqs1A1uDfIrIe+LNInMWXkpJCa+voM3qam5vJyMggKSmJPXv28Oabb05y64wZR/190HJs9AA6WeEmJQRLSHeBk7cY5l8fFEAlkDbDutrMhDhrQKlqn4h8EXgRN838EVXdKSJ/A2xW1WcnupGTxe/3s3r1ahYtWkRiYiK5ublD29asWcODDz7I/PnzmTdvHitXrgxhS405i8GZb6NWQJXQdHT4iaRRMW61goxSKFgWCKD0EhdCiRmT/x3MtCeqoRkKWr58uY68YOHu3buZP39+SNoTCtPt+5oJ0NMOdXuhdjfU7QkKospTFwdNyhrR/VYSeJxaGJZrsZnpQUS2qOrykc/bShLGRIL+Xmg4ACd2ujCq3Q21u1wY4f0jMzouEDgzLjs1jOJTQtR4Y86PBZQx4WRgAJoqAgE0GEb1+wMTEyQa/LPd4qFLboGc+ZCzADLLrAoyU4oFlDGhoAqt1cNDqHaX667r7Qjsl17swmfuh919znw3VTsm/vTvbcwEUlXauvtoaOtBgbIs31lfc74soIyZaB2Np1ZEtbvc1UsHJee68Lnks4GKKHuedcuZSdHTN0Bjew/1bd00tPfQ0NZNQ1sP9e3uvmHoebdPd98AAKtn+3nsCxM3YcwCypjx0tMOtXuCgsi7b6sJ7BOf5gJo4UdcCOUugOz57mRVY8aJqtLS2UddW3dQuHRT39ZDw1DoBAKoubN31PeJi47Cnxznbr54Zuckk50cP/R3iX9iT6y2gDLmXPX1QMP+EVXR4IQFT0wCZJfDrA8EKqKc+ZBaYCetmvPS1dtPQ3sP9a3dNLR7YRNU3dR7Vc9gAPUNjD5DOyMpFn9yPH5fHPPzUvEnx5EVFDpZyXFue3IcKfExIb12nQXUBUhOTqatrS3UzTAToa/brZgwOG278bB3f9DNphtcvFSiIWuOO3do6a2BMMootQkL5qx6+gaob+umrtW7BT/2/h4Mnrbu0RfMTYiN8gImnvy0BBYVpg4FUHZKPH5f/FAVlJkUR0x05CyYawFlpidVNzZ00guek4ehsSIQSC3HGZq+De5KqBmlkDkTyq8NVET+2TZhwQwzMKA0dfZS29o1PGxGBlBbN00do3etpSfFkp0cT1ZyPEuK0gNVji9Q7QzeJ8VN3Z/xqfvNzsO9997LjBkzuOuuuwC47777iImJYd26dZw8eZLe3l6+9a1vceONN4a4pWZM+nqg+WhQCFV4lZC3xtzIC9Ul57kQKr3CC6OywHlEybnWNTfNtXf3jV7ljHiuvq171O61hNgoclISyE6JZ1Z2Mitn+slOiXe35Pihx/7kOOJjrPqGcA6oF+6Fmu3j+555i2HtqBcDBuDmm2/mnnvuGQqoJ598khdffJEvfelLpKamUl9fz8qVK7nhhhtC2i9rPMOW8xklhFqOgQ4E9o+ODwROyarhAZReYitpT0OqSl1rNydauqlrG17x1I4In46e/lNeHx0l+H1x5KS6kJmfnxIUOAmBAEqJxxcXbb8b5yh8AyoEli1bRm1tLVVVVdTV1ZGRkUFeXh5f/vKXeeWVV4iKiuL48eOcOHGCvLy8UDd3eujvheZjpw+hkcv5+HJc4BSvHKUKyrML1k1Dqkp9Ww8VDe0crm+nor7de9xBZUP7qMGTlhg7FDRLitJHrXSyU+LJSIojOspCZ6KEb0CdodKZSJ/4xCd46qmnqKmp4eabb+axxx6jrq6OLVu2EBsbS2lp6aiX2TAXqKfdVczV7wZmxDUeduEUvKhpdJy3gGlpYDmfwRBKL4H45NC034SUqtLY3jMUPBX17RxucGFU2dAxbIJBTJRQnJlEaZaPlTMzKcvykZ+WOBQ6WdbFFjbCN6BC5Oabb+aOO+6gvr6eDRs28OSTT5KTk0NsbCzr1q2jsrIy1E2MfN2tUP2eC6PqbVC1zU3bHuyOS8x0kxGKLoXFnxgeQin5NjtuGjvZ3jMUPBUNHUHVUDutXYEQio4SijISKfX7uLQ0k1K/C6SyLB+F6YkRNZNtOrOAGmHhwoW0trZSWFhIfn4+t956K9dffz2LFy9m+fLllJeXh7qJkaWr2Quid10QVb/rpmkPzpBLyYf8pe7E1YKl7nFKnk1ImMaaO3uHBY+rhlwYBZ9QGiVQ6IXQR5YVUur3UZqVRKnfx4zMJGIthCKeBdQotm8PTM7Iyspi48aNo+5n50CN0NEINe95QbTNhVHjocD21CIXQhd90gVR/hJIyR39vcyU1trVS0V9R6Aa8rrkKhs6aGzvGdpPBArSEinNSuK6i/Ipy/J5QeRjRmaidcVNcRZQ5vy0N3ghtC1QGTUFdX+mF7sAWnproDLyZY36VmZqau7opbLRhc6Rxo5hExTq23qG7ZuflkCp38eHF+ZR5lVBZVmuEkqItRCariygzNm11QZ10W1zj5uPBrZnlLmVFJZ/LlAZJWWGqLFmsgwMKLWt3VQ2tFPZ2MGRhg7v3o0PjVzfLTc1nhK/j6vKc73xIDcuVJLpIzHOQsicKuwCSlWnxbkCobqS8Vm11gwPoqpt0FoV2O6fDTNWwIo7XRDlL4HE9NC01Uy43v4Bjp/spKKhnSONHVQ2dHgVkfu7qzdwnll0lFCYnkiJ33XHlfiTKPH7KPEnUZyZNKVXPDATI6z+i0lISKChoQG/3z+lQ0pVaWhoICEhIZSNgJaq4UFUvQ3aTng7iLvuUOkVXhfdEsi7CBJSQ9ZkMzHau/u88HHdcYFqqJ2qpi76g1ZFSIiNoiTTR4nfx/vmZLvw8fso9SdRkJ5oExPMuAqrgCoqKuLYsWPU1dWFuikTLiEhgaKiosn7wL5uF0JH3oAjb8KxzdBR77ZJFGTNcytvD3bR5S22c4qmiMFzhAaDp6Khfag7rrKhg/q27mH7ZyTFUuz3sWxGBjctddVPiRdC2SnxU/ofjya8hFVAxcbGUlZWFupmTA2dTXD0bTiy0QXS8S3Q7/0QZc11V2jNX+qqo9xFtsxPhBsYUKqaO4cFz2BFdKRx+ImqIpCXmkCJP4mrynMo9ie57rhMH8X+JNISY0P4TYwJCKuAMheg+bgXRl4gndgJKETFuCBacQcUX+6WALLZdBFJ1U1KOFTnZsJV1LvzhA7Xu0kKPX2B8aDYaGFGhgueFWWZXhXkbkUZNjPORIYxBZSIrAG+A0QDD6vq/SO2/wlwF9APtAF3ququcW6rGTQwAHV7AmF05E1oPuK2xSW7SQwLbnRhVHgJxPlC214zZqpKQ3vPUPgMnqw62rpxcTFRlHhL9vxheY47P8ifRLE/ify0RFsjzkS8swaUiEQD3wc+CBwDNonIsyMC6Geq+qC3/w3AA8CaCWjv9NTXDVXvDA+kria3LTnXVUaX3+UCKXcRRFthHO6aO3qHTlI9NGwB0+FL9sRECTMykyj1J3H5TP/Q1OxSv4+CdAshM7WN5ZdsBXBAVQ8BiMgTwI3AUECpakvQ/j6GXenNnLOzjR8tuCHQXZdRZssCham27r5AJeStlDD4+GTQhepEoDA9kbKswJI9ZYPrxmXYzDgzfY0loAqBoLMyOQZcNnInEbkL+AoQB3xgtDcSkTuBOwGKi4vPta1TV/MxrzLaCJUb3WreNn4UEbp6+4PGgzo4XN82tIRPXevw2XGDqyWsWZRPWVYSZVnJlGUlMSMzyZbsMWYU49YXpKrfB74vIp8Gvg58ZpR9HgIeAli+fPn0rLJOGT/aGFiVYXD8aOFN3vjRcptdFwYGJyfsqWll/4nWQJdcfTtVzcMvvZKVHE9ZVhJXzs2mLNtHmbduXKnfVksw5lyNJaCOAzOC/i7ynjudJ4B/u5BGTSljGj/6oo0fhYmOnj72nWhjb00Lu6tb2VPTwt6a1mFdculJsZT6fayc6XfhkzUYREmkJNgUbWPGy1h+DTcBc0SkDBdMnwI+HbyDiMxR1f3en9cC+5nOTlbCez+Hg7+H41tPM350ubu+kY0fhUT/gHKksWMoiPbWuDCqbOxgcBWqpLho5uamsGZRHvNyUyjPT2VebgoZvrjQNt6YaeKsAaWqfSLyReBF3DTzR1R1p4j8DbBZVZ8FvigiVwO9wElG6d6b8no6YPez8M6jUPEqIFB4sY0fhYHG9p6hSmiPVxXtO9FGZ6+bsi0CZX4f8/NT+ciyIsrzUyjPS2FGRhJRNkvOmJCRUC1aunz5ct28eXNIPnvcqMLRt2DbY7DjV9DT6qqipbfCkk+5S06YSdPd18+B2javGvJu1S3UBk1WyPTFUZ6XQnleqrvPT2FOToqNDxkTQiKyRVWXj3zeBjzOR0sVvPs4bPuZuzpsrM9NbFj6aSheBVE2LXgiqSpVzV3sqW4ZFkSH6tuHFjaNi45iTm4yV8zJYn5eKuX5KczLSyE72daSMyZSWECNVW8X7P0NvPMYHFoHOuDC6Iovu1Ub4lNC3cIpqbWrN6giahl6HHwya1FGIuV5KXx4YR7z8lKYn59Cqd9HjJ0/ZExEs4A6E1Wo2uoqpe1Pudl3qUXwB1+FJbeAf1aoWzhl9PUPUNHQPmzCwp6aVo6d7BzaJyU+hvL8FG5cWjDURTc3L4VUmzlnzJRkATWatlo3C++dx6BuN8QkwPzrXRde2fshysYrLkR9W/fQZIXBymjfibahxU6jo4SZWT6WFWdwy4pib6wolYK0BOueM2YasYAa1NcD+1901dK+F0H73Ymy1/0rLPyoXTX2PHT1ukkLg2NEe0+0sru6ddj1h7JT4inPS+Gzq0q9qdwpzM5JtpUVjDEWUNTscLPw3vs5dDS4k2dXfdHNxMueF+rWRYSRkxZ2e/eHgyYtxMdEMTc3hT+cl015firz89ykBX9yfIhbb4wJV9MzoDoaYfsvXDBVvwtRsVB+jQulWVfZag5n0NbdFxgjCuqmO3XSQiprF+W5sSJv0oKtvG2MORfT55e4vw8O/s6F0t4XoL8H8i6Ctf8Iiz8BSZmhbmFY6R9QKhra2VPd6lZb8ELpaOPpJy3Mz09hbm6KLfdjjBkXUz+g6vbBtkfh3Z9DWw0k+eHSL7gJD3mLQ926sNDY3sOeahdCe72KaN+JVrp63aSFKIGZ2cksKUrnU5cWD40VFaYn2qQFY8yEmZoB1dUMO37pJjwc2wQSDXM/7Lrw5nwIYqbvWmqN7T28eaiBbUebRl1pISs5jvK8VG67rITyfDeVe3ZOsl0i3Bgz6aZOQA0MwOH1LpR2Pwd9XZA9Hz70LbjoZkjOCXULQ6Kjp4+3DzfyxsEGXj9Qz67qFlTd5cLn5CTzB3OymZ/vlv6Zl5dCdopNWjDGhIfID6jGQy6Utj0OLccgIQ2W3eaqpYJl02618N7+AbYdbeL1A/W8caCBd46epLdfiYuO4uKSdL5y9VxWzc7ioqI0u1KrMSasRW5A9XTAYx+HytdBomDWB+BDfwvzroHYhFC3btIMDCi7a1p440ADrx+s5+3DjXT09CMCiwrS+PwVM1k928/ykkxbENUYE1EiN6DikiC1AK76hlt2KLUg1C2aFKruOkave4G08WADje09AMzM9vGxi4tYPdvPypl+0pOm71ibMSbyRW5AAXzs4VC3YFLUtnax0RtDev1AA8eb3FTvvNQErpyXzepZWaya7Sc/LTHELTXGmPET2QE1RbV09fLWoUY3jnSwnn0n2gBITYhh1aws/uT9M1k1O4uZWT6b5m2MmbIsoMJAV28/W4+c5I0DDbx2oJ7tx5vpH1ASYqO4tDSTjyxz3XYLC9JsNQZjzLRhARUC/QPKjuPNvH7QzbTbVNFId98A0VHCkqI0/vTKWayalcXFJem2aKoxZtqygJoEqsrBujY3seFAPW8eaqDFW7uuPC+FWy8rYfVsPyvKMm2ZIGOM8VhATZD6tm7W763jjQP1vH6wnhMtbrWGooxE1i7KZ9VsP6tmZdmJscYYcxoWUOOsrrWbBzcc5NE3K+nuG8Dvi+PyWX5Wz85i9awsiv1JoW6iMcZEBAuocdLY3sO/v3KQn7xRSU//AB9dVshnV5cyPy+VKJvYYIwx52xMASUia4DvANHAw6p6/4jtXwG+APQBdcAfq2rlOLc1LDV19PDwq4f58euH6ezt58alhXzpqjmUZflC3TRjjIloZw0oEYkGvg98EDgGbBKRZ1V1V9Bu7wDLVbVDRP4n8I/AzRPR4HDR0tXLj149zCOvHaatp49rF+dzz9VzmJ2TEuqmGWPMlDCWCmoFcEBVDwGIyBPAjcBQQKnquqD93wRuG89GhpO27j7+4/XDPPTKIVq6+li7KI+7r55DeV5qqJtmjDFTylgCqhA4GvT3MeCyM+z/eeCF0TaIyJ3AnQDFxcVjbGJ46Ojp4ycbK/n3DQc52dHL1fNzuefqOSwqTAt104wxZkoa10kSInIbsBx4/2jbVfUh4CGA5cuX63h+9kTp6u3n0TcreXDDQerberhyXjZfvnouS2akh7ppxhgzpY0loI4DM4L+LvKeG0ZErgb+N/B+Ve0euT3SdPf188TbR/n+ugPUtnZzxewsvvzBOVxSkhnqphljzLQwloDaBMwRkTJcMH0K+HTwDiKyDPh3YI2q1o57KydRT98Av9hylO/9/gDVzV1cVpbJ/7tlGZfN9Ie6acYYM62cNaBUtU9Evgi8iJtm/oiq7hSRvwE2q+qzwD8BycAvvNW1j6jqDRPY7nHX2z/A01uP8d3fHeB4UyeXlGTwL59YwuWz/LZiuDHGhMCYxqBU9Xng+RHPfSPo8dXj3K5J09c/wDPbqvju7/dT2dDBkqI0/u6ji3nfnCwLJmOMCaFpu5JE/4Dy6/eq+M7v9nOorp2FBan86DPL+UB5jgWTMcaEgWkXUAMDyn/vrOHbL+9j34k2yvNSePC2S/jwwlwLJmOMCSPTJqBUlZd2neBfX97P7uoWZuck871PL+OaRfm2Vp4xxoShKR9Qqsr6vXU88NI+th9vpizLx7dvXsr1Swrs6rTGGBPGpmxAqSqv7q/ngZf2se1oEzMyE/mnj1/ER5YVEhMdFermGWOMOYspGVBvHKznX1/ax6aKkxSmJ3L/RxfzsUuKiLVgMsaYiDGlAmpTRSMP/HYfGw81kJeawN/etIhPLi8iPiY61E0zxhhzjqZEQG09cpJ/fWkfr+6vJys5nm9ev4BbVhSTEGvBZIwxkSqiA2r7sWYeeGkv6/bWkemL439fM5/bVpaQGGfBZIwxkS5iA6qtu49PPbSR2Jgo/mLNPD5zeSm++Ij9OsYYY0aI2F/05PgYfvhHy1lclEZKQmyom2OMMWacRWxAAayanRXqJhhjjJkgNu/aGGNMWLKAMsYYE5ZENTRXXheROqByHN4qC6gfh/eZbuy4nR87bufPjt35mQ7HrURVs0c+GbKAGi8isllVl4e6HZHGjtv5seN2/uzYnZ/pfNysi88YY0xYsoAyxhgTlqZCQD0U6gZEKDtu58eO2/mzY3d+pu1xi/gxKGOMMVPTVKigjDHGTEEWUMYYY8JSxAaUiKwRkb0ickBE7g11eyKFiMwQkXUisktEdorI3aFuUyQRkWgReUdEfh3qtkQKEUkXkadEZI+I7BaRy0PdpkggIl/2/j+6Q0QeF5GEULdpskVkQIlINPB9YC2wALhFRBaEtlURow/4qqouAFYCd9mxOyd3A7tD3YgI8x3gv1W1HFiCHb+zEpFC4EvAclVdBEQDnwptqyZfRAYUsAI4oKqHVLUHeAK4McRtigiqWq2qW73Hrbgfi8LQtioyiEgRcC3wcKjbEilEJA14H/AjAFXtUdWmkDYqcsQAiSISAyQBVSFuz6SL1IAqBI4G/X0M+5E9ZyJSCiwD3gpxUyLFt4G/AAZC3I5IUgbUAT/2ukYfFhFfqBsV7lT1OPDPwBGgGmhW1d+GtlWTL1IDylwgEUkGfgnco6otoW5PuBOR64BaVd0S6rZEmBjgYuDfVHUZ0A7YmPFZiEgGrleoDCgAfCJyW2hbNfkiNaCOAzOC/i7ynjNjICKxuHB6TFWfDnV7IsRq4AYRqcB1KX9ARB4NbZMiwjHgmKoOVulP4QLLnNnVwGFVrVPVXuBpYFWI2zTpIjWgNgFzRKRMROJwg4fPhrhNEUFEBDcesFtVHwh1eyKFqv6Vqhapainuv7ffq+q0+xftuVLVGuCoiMzznroK2BXCJkWKI8BKEUny/j97FdNwcklEXlFXVftE5IvAi7jZLY+o6s4QNytSrAZuB7aLyDbvua+p6vOha5KZ4v4X8Jj3j8lDwOdC3J6wp6pvichTwFbczNt3mIZLHtlSR8YYY8JSpHbxGWOMmeIsoIwxxoQlCyhjjDFhyQLKGGNMWLKAMsYYE5YsoIwxxoQlCyhjjDFh6f8D7/P9VoISf9UAAAAASUVORK5CYII=","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["num_epochs = 10\n","model_name = 'custom CNN'\n","\n","loss_fn = nn.CrossEntropyLoss() # already includes the Softmax activation\n","optimizer_custom = torch.optim.SGD(custom_cnn.parameters(), lr=1e-3)\n","\n","custom_train_history, custom_val_history = train(custom_cnn, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_custom)\n","\n","plotTrainingHistory(custom_train_history, custom_val_history)"]},{"cell_type":"markdown","metadata":{"id":"cZSe3Knytp5A"},"source":["### 7x7 versus 3x3 Convolutions\n","\n","Historically, the VGG was the first CNN architecture to introduce more layers (16-19 layers versus the 8 layers of AlexNet) and smaller convolutional kernel sizes (3x3).\n","\n","This is mainly due to the fact that a stack of 3 3x3 convolutional layers (with stride 1) has the same effective receptive field as a single 7x7 layer. Why?\n","\n","You can find out more about the effective receptive field of CNNs and explore some visualizations [here](https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n","\n","Compare the number of parameters of a single convolutional layer with 7x7 kernel with a stack of 3 convolutional layers with 3x3 kernels."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"6hg9leFzuRf3"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 1, 30, 30]              10\n","================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.01\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.01\n","----------------------------------------------------------------\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 30, 30]             896\n","              ReLU-2           [-1, 32, 30, 30]               0\n","            Conv2d-3           [-1, 32, 28, 28]           9,248\n","              ReLU-4           [-1, 32, 28, 28]               0\n","         MaxPool2d-5           [-1, 32, 14, 14]               0\n","           Dropout-6           [-1, 32, 14, 14]               0\n","           Flatten-7                 [-1, 6272]               0\n","            Linear-8                   [-1, 64]         401,472\n","              ReLU-9                   [-1, 64]               0\n","          Dropout-10                   [-1, 64]               0\n","           Linear-11                   [-1, 10]             650\n","================================================================\n","Total params: 412,266\n","Trainable params: 412,266\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.97\n","Params size (MB): 1.57\n","Estimated Total Size (MB): 2.55\n","----------------------------------------------------------------\n"]}],"source":["# TODO\n","# consider convolutional layers with 1 input and output channels\n","# hint: use the summary method from the torchsummary package\n","\n","m1 = nn.Sequential(nn.Conv2d(1,1,kernel_size=3, padding=0)).to(device)\n","summary(m1,(1,32,32))\n","\n","m2 = nn.Sequential(nn.Conv2d(1,1,kernel_size=3, padding=0),\n","                   nn.Conv2d(1,1,kernel_size=(3,3), padding=0),\n","                   nn.Conv2d(1,1,kernel_size=(3,3), padding=0)\n","                   ).to(device)\n","\n","#summary(m2,(1,32,32))\n","\n","summary(custom_cnn,(3,32,32))"]},{"cell_type":"markdown","metadata":{"id":"7Lc1auKjyMfr"},"source":["What are the advantages of using a stack of 3 convolutional layers with 3x3 kernels instead of a single 7x7 layer?"]},{"cell_type":"markdown","metadata":{"id":"0aqWR_0VzRED"},"source":["### VGG model\n","\n","Finetune a VGG-16 model.\n","\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"DfK3c9RSzRED"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\radio/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n","  1%|          | 4.55M/528M [00:46<1:29:35, 102kB/s]  \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\radio\\OneDrive\\Ambiente de Trabalho\\PwoGwammingUwU\\VC-EXERCICIOS\\tutorial-week1\\cnn_architectures.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TODO\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# load model from torchvision (with pretrained=True)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vgg \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39;49mvgg16(pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(vgg)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# change the number of neurons in the last layer to the number of classes of the problem at hand (CIFAR10 dataset)\u001b[39;00m\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m{\u001b[39;00msequence_to_str(\u001b[39mtuple\u001b[39m(keyword_only_kwargs\u001b[39m.\u001b[39mkeys()), separate_last\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mand \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m as positional \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[39m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m builder(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchvision\\models\\vgg.py:433\u001b[0m, in \u001b[0;36mvgg16\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m\"\"\"VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[0;32m    415\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m    :members:\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m weights \u001b[39m=\u001b[39m VGG16_Weights\u001b[39m.\u001b[39mverify(weights)\n\u001b[1;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m _vgg(\u001b[39m\"\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m, weights, progress, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchvision\\models\\vgg.py:105\u001b[0m, in \u001b[0;36m_vgg\u001b[1;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m model \u001b[39m=\u001b[39m VGG(make_layers(cfgs[cfg], batch_norm\u001b[39m=\u001b[39mbatch_norm), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(weights\u001b[39m.\u001b[39;49mget_state_dict(progress\u001b[39m=\u001b[39;49mprogress))\n\u001b[0;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m model\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchvision\\models\\_api.py:89\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, progress)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_dict\u001b[39m(\u001b[39mself\u001b[39m, progress: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Mapping[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m load_state_dict_from_url(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, progress\u001b[39m=\u001b[39;49mprogress)\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\hub.py:746\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    744\u001b[0m         r \u001b[39m=\u001b[39m HASH_REGEX\u001b[39m.\u001b[39msearch(filename)  \u001b[39m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         hash_prefix \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m r \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 746\u001b[0m     download_url_to_file(url, cached_file, hash_prefix, progress\u001b[39m=\u001b[39;49mprogress)\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    749\u001b[0m     \u001b[39mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\hub.py:633\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mfile_size, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m progress,\n\u001b[0;32m    631\u001b[0m           unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit_divisor\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m    632\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 633\u001b[0m         buffer \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39;49mread(\u001b[39m8192\u001b[39;49m)\n\u001b[0;32m    634\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    635\u001b[0m             \u001b[39mbreak\u001b[39;00m\n","File \u001b[1;32mc:\\Python38\\lib\\http\\client.py:454\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 454\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n","File \u001b[1;32mc:\\Python38\\lib\\http\\client.py:498\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    493\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    495\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    500\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n","File \u001b[1;32mc:\\Python38\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Python38\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n","File \u001b[1;32mc:\\Python38\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# TODO\n","# load model from torchvision (with pretrained=True)\n","vgg = models.vgg16(pretrained=True)\n","print(vgg)\n","\n","# change the number of neurons in the last layer to the number of classes of the problem at hand (CIFAR10 dataset)\n","vgg.classifier[6] = nn.Linear(4096, 10)\n","\n","vgg.to(device)\n","print(vgg)"]},{"cell_type":"markdown","metadata":{"id":"JX-vNZF3fiqs"},"source":["Inspect the model structure.\n","\n","What does the AdaptiveAvgPool2d layer do?\n","\n","What is the shape of the feature map before and after this layer?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAl5WoxWlwcK"},"outputs":[],"source":["num_epochs = 10\n","model_name = 'vgg16'\n","\n","loss_fn = nn.CrossEntropyLoss() # already includes the Softmax activation\n","optimizer_vgg = torch.optim.SGD(vgg.parameters(), lr=1e-3)\n","\n","vgg_train_history, vgg_val_history = train(vgg, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_vgg)\n","\n","plotTrainingHistory(vgg_train_history, vgg_val_history)"]},{"cell_type":"markdown","metadata":{"id":"aDzxzS63q4AP"},"source":["### ResNet Model\n","Repeat the finetuning process with a ResNet-50 model."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"MTZd8yFhrL9b"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",")\n"]}],"source":["# TODO\n","# load model from torchvision (with pretrained=True)\n","resnet = models.resnet50(pretrained=True)\n","\n","# change the number of neurons in the last layer to the number of classes of the problem at hand (CIFAR10 dataset)\n","resnet.fc = nn.Linear(2048, 10)\n","\n","resnet.to(device)\n","print(resnet)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"oMlKInxPrgS-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training...\n","\n","Epoch 1\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 22/781 [00:43<24:55,  1.97s/it] \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\radio\\OneDrive\\Ambiente de Trabalho\\PwoGwammingUwU\\VC-EXERCICIOS\\tutorial-week1\\cnn_architectures.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# already includes the Softmax activation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m optimizer_resnet \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(resnet\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m resnet_train_history, resnet_val_history \u001b[39m=\u001b[39m train(resnet, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_resnet)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plotTrainingHistory(resnet_train_history, resnet_val_history)\n","\u001b[1;32mc:\\Users\\radio\\OneDrive\\Ambiente de Trabalho\\PwoGwammingUwU\\VC-EXERCICIOS\\tutorial-week1\\cnn_architectures.ipynb Cell 22\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m epoch_iter(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Train acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m epoch_iter(validation_dataloader, model, loss_fn, is_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","\u001b[1;32mc:\\Users\\radio\\OneDrive\\Ambiente de Trabalho\\PwoGwammingUwU\\VC-EXERCICIOS\\tutorial-week1\\cnn_architectures.ipynb Cell 22\u001b[0m in \u001b[0;36mepoch_iter\u001b[1;34m(dataloader, model, loss_fn, optimizer, is_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m is_train:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Save training metrics\u001b[39;00m\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["num_epochs = 10\n","model_name = 'resnet50'\n","\n","loss_fn = nn.CrossEntropyLoss() # already includes the Softmax activation\n","optimizer_resnet = torch.optim.SGD(resnet.parameters(), lr=1e-3)\n","\n","resnet_train_history, resnet_val_history = train(resnet, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_resnet)\n","\n","plotTrainingHistory(resnet_train_history, resnet_val_history)"]},{"cell_type":"markdown","metadata":{"id":"u6pf9u38zADi"},"source":["### Compare the VGG and ResNet models in terms of:\n","\n","\n","1.   number of parameters\n","2.   validation accuracy\n","3.   training time\n","\n","What is the main difference introduced by the ResNet architecture?\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"fEtXUkArZx8J"},"outputs":[{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (2x2048 and 512x10)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\radio\\OneDrive\\Ambiente de Trabalho\\PwoGwammingUwU\\VC-EXERCICIOS\\tutorial-week1\\cnn_architectures.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TODO\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m summary(resnet,(\u001b[39m3\u001b[39;49m,\u001b[39m32\u001b[39;49m,\u001b[39m32\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/cnn_architectures.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m summary(vgg,(\u001b[39m3\u001b[39m,\u001b[39m32\u001b[39m,\u001b[39m32\u001b[39m))\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[0;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torchvision\\models\\resnet.py:280\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n","File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2048 and 512x10)"]}],"source":["# TODO\n","summary(resnet,(3,32,32))\n","summary(vgg,(3,32,32))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Challenges (closer to the project)\n","Adapt the previous model, or use any of the well-known CNN architectures, to tackle one of (or both) these competitions, that consist of a binary medical image classification problem.\n","\n","a) Identify metastatic cancer in small image patches (32x32) taken from larger digital pathology scans\n","https://www.kaggle.com/c/histopathologic-cancer-detection\n","\n","b) Classify CT images into positive or negative COVID-19, depending if the image has or does not have clinical findings of COVID-19 https://covid-ct.grand-challenge.org/Data/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"cnn_architectures_pytorch.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}
