{"cells":[{"cell_type":"markdown","metadata":{"id":"oOsJ5IjcAVE-"},"source":["# Semantic Segmentation\n","\n","In this tutorial we will fine tune a U-NET model for semantic segmentation on the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8ppo-YL4tmx1"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","\"$(pip freeze | grep albumentations) is successfully installed\"\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Ignoring invalid distribution -rotobuf (c:\\python38\\lib\\site-packages)\n","WARNING: Skipping opencv-python-headless as it is not installed.\n"]}],"source":["!pip install -q -U albumentations\n","!echo \"$(pip freeze | grep albumentations) is successfully installed\"\n","!pip uninstall -y opencv-python-headless\n","!pip install opencv-python-headless==4.1.2.30"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"soMygqHFr_-i"},"outputs":[],"source":["import cv2\n","import os\n","from tqdm import tqdm\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torchsummary import summary"]},{"cell_type":"markdown","metadata":{"id":"XNvgtC5RBEFQ"},"source":["## Data Preparation"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WIeJGWYJVSDR"},"outputs":[{"name":"stderr","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100   350  100   350    0     0    614      0 --:--:-- --:--:-- --:--:--   618\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100   355  100   355    0     0   1289      0 --:--:-- --:--:-- --:--:--  1295\n","tar: Error opening archive: Unrecognized archive format\n","tar: Error opening archive: Unrecognized archive format\n"]}],"source":["# let's start by downloading the necessary data\n","!curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n","!curl -O https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n","!tar -xf images.tar.gz\n","!tar -xf annotations.tar.gz"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IKKzhOiIr3Y0"},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] O sistema não conseguiu localizar o caminho especificado: 'images'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\radio\\OneDrive\\Ambiente de Trabalho\\PwoGwammingUwU\\VC-EXERCICIOS\\tutorial-week1\\semantic_segmentation.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/semantic_segmentation.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m images_directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/semantic_segmentation.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m masks_directory \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrimaps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/semantic_segmentation.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m images_filenames \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(images_directory)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/semantic_segmentation.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# filter out images that can not be loaded properly\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/radio/OneDrive/Ambiente%20de%20Trabalho/PwoGwammingUwU/VC-EXERCICIOS/tutorial-week1/semantic_segmentation.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m correct_images_filenames \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m images_filenames \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mimread(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(images_directory, i)) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] O sistema não conseguiu localizar o caminho especificado: 'images'"]}],"source":["# get train and validation datasets\n","images_directory = \"images\"\n","masks_directory = os.path.join(\"annotations\", \"trimaps\")\n","\n","images_filenames = list(sorted(os.listdir(images_directory)))\n","# filter out images that can not be loaded properly\n","correct_images_filenames = [i for i in images_filenames if cv2.imread(os.path.join(images_directory, i)) is not None]\n","\n","random.seed(42)\n","random.shuffle(correct_images_filenames)\n","\n","train_images_filenames = correct_images_filenames[:2000]\n","val_images_filenames = correct_images_filenames[2000:2500]\n","\n","print(len(train_images_filenames), len(val_images_filenames))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faXhXSbUs1T4"},"outputs":[],"source":["# the dataset has 3 labels (1 - pet, 2 - background and 3 - border), so we convert it to a binary problem with (0 - background and 1 - pet/border)\n","def preprocess_mask(mask):\n","    mask = mask.astype(np.float32)\n","    mask[mask == 2.0] = 0.0\n","    mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-1LzO8Vs451"},"outputs":[],"source":["# before proceeding, let us visualize some examples\n","def display_image_grid(images_filenames, images_directory, masks_directory):\n","    rows = len(images_filenames)\n","    figure, ax = plt.subplots(nrows=rows, ncols=2, figsize=(10, 24))\n","    for i, image_filename in enumerate(images_filenames):\n","        image = cv2.imread(os.path.join(images_directory, image_filename))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,)\n","        mask = preprocess_mask(mask)\n","        ax[i, 0].imshow(image)\n","        ax[i, 1].imshow(mask, interpolation=\"nearest\", cmap='gray')\n","\n","        ax[i, 0].set_title(\"Image\")\n","        ax[i, 1].set_title(\"Ground truth mask\")\n","\n","        ax[i, 0].set_axis_off()\n","        ax[i, 1].set_axis_off()\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Tf4Rsm5s969"},"outputs":[],"source":["display_image_grid(train_images_filenames[:5], images_directory, masks_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdoRdGNftYLX"},"outputs":[],"source":["# let's define our dataset\n","class OxfordPetDataset(Dataset):\n","    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n","        self.images_filenames = images_filenames\n","        self.images_directory = images_directory\n","        self.masks_directory = masks_directory\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images_filenames)\n","\n","    def __getitem__(self, idx):\n","        image_filename = self.images_filenames[idx]\n","        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        # transform image into 0-1 range\n","        # note that the ToTensorV2 method from the albumentations library does not automatically convert the image range into 0-1\n","        image = image / 255.\n","\n","        mask = cv2.imread(\n","            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,\n","        )\n","        mask = preprocess_mask(mask)\n","\n","        # when applying data augmentation we need to make sure both input image and target mask are augmented using exactly the same augmentation\n","        # this is straightforward using the albumentations library (https://github.com/albumentations-team/albumentations)\n","        if self.transform is not None:\n","            transformed = self.transform(image=image, mask=mask)\n","            image = transformed[\"image\"]\n","            mask = transformed[\"mask\"]\n","        return image.float(), mask.to(torch.int64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJz6Z_UAudIw"},"outputs":[],"source":["# hyperparameters\n","nr_classes = \n","batch_size = \n","num_workers = \n","epochs = \n","learning_rate = \n","\n","# device\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iE2SnTPquWuc"},"outputs":[],"source":["# define transforms and datasets\n","train_transform = A.Compose(\n","    [\n","        A.Resize(256, 256),\n","        A.RandomCrop(224, 224),\n","        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n","        ToTensorV2(),\n","    ]\n",")\n","train_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","\n","val_transform = A.Compose(\n","    [A.Resize(256, 256), A.CenterCrop(224, 224), ToTensorV2()]\n",")\n","val_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"etItv_MNBY2a"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1QZ4C7euufx"},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","\n","        # if bilinear, use the normal convolutions to reduce the number of channels\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","            self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class SegmentationNetwork(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        super(SegmentationNetwork, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        factor = 2 if bilinear else 1\n","        self.down4 = Down(512, 1024 // factor)\n","        self.up1 = Up(1024, 512 // factor, bilinear)\n","        self.up2 = Up(512, 256 // factor, bilinear)\n","        self.up3 = Up(256, 128 // factor, bilinear)\n","        self.up4 = Up(128, 64, bilinear)\n","        self.outc = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"jp-CQc-_7ZPw"},"source":["Analyse the implemented SegmentationNetwork. \n","\n","What type of architecture is being implemented? \n","\n","What is the shape of the output and embedding tensors?"]},{"cell_type":"markdown","metadata":{"id":"9M5_1a5TBhvb"},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7pkokMlWSPt"},"outputs":[],"source":["# create model\n","model = # TODO\n","print(model)\n","\n","# put model in GPU\n","model.to(device)\n","\n","# define optimizer\n","optimizer = # TODO\n","\n","# define loss\n","loss_fn = # TODO\n","\n","# define metric (e.g. Jaccard Index, Dice Coefficient, Pixel Accuracy)\n","metric = # TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWyLBQ3c41BY"},"outputs":[],"source":["# train the model (do not forget to save your best model during training)\n","# TODO"]},{"cell_type":"markdown","metadata":{"id":"m7Up7KWSBzLf"},"source":["## Test the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3HmvIW22R-Q"},"outputs":[],"source":["# test the model on some images from the validation dataset\n","# compare the predicted with the ground-truth masks\n","# TODO"]},{"cell_type":"markdown","metadata":{"id":"cImzPTrEEu4z"},"source":["## Challenge\n","Add the Dice loss to the training of the network and compare the results."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"semantic_segmentation_pytorch.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"metadata":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
